---
title: "Finals"
author: "Tanush Shetty"
email : "ts1333@scarletmail.rutgers.edu"
date: "09/12/2024"
output: html_document
---

```{r}
library(fpp)
library(fpp2)
library(dplyr)
library(forecast)
library(readr)

# Import data
car_sales_data <- read_csv("C:/Users/tanus/Downloads/TOTALSA.csv", col_names = FALSE)

# Rename columns
colnames(car_sales_data) <- c("Date", "Sales")

print("Column Names:")
print(names(car_sales_data)) 

# Remove leading and trailing whitespace from Date values (if any)
car_sales_data$Date <- trimws(car_sales_data$Date)

# Convert Date column to Date type with proper format
car_sales_data$Date <- as.Date(car_sales_data$Date, format="%m/%d/%Y")

str(car_sales_data)

head(car_sales_data)

car_sales_data <- na.omit(car_sales_data)


# Sort the data by date (in case it's not in chronological order)
car_sales_data <- car_sales_data[order(car_sales_data$Date), ]


# Check start date to ensure no NA is present
start_year <- as.numeric(format(min(car_sales_data$Date, na.rm=TRUE), "%Y"))
start_month <- as.numeric(format(min(car_sales_data$Date, na.rm=TRUE), "%m"))

# Verify that start_year and start_month are not NA
if (is.na(start_year) | is.na(start_month)) {
  print(paste("Start year is:", start_year, "and start month is:", start_month))
  stop("Start year or start month is NA. Check the Date column for missing or incorrect values.")
}

# Create a time series object for the 'Sales' column
# Assumption: Data is monthly, so we set frequency = 12 (12 months in a year)
car_sales_ts <- ts(car_sales_data$Sales, 
                   start = c(start_year, start_month), 
                   frequency = 12)

# Plot the time series to visualize the trend and seasonality
plot(car_sales_ts, main="U.S. Car Sales (in Millions)", xlab="Year", ylab="Sales (Millions)")

# Print a summary of the time series to check its structure
summary(car_sales_ts)

# Filter Post-2021 Data:
# 
# Focus on data from January 2021 onward to capture the stabilized period after the pandemic's initial shock.
# Use this filtered dataset to build and validate your forecast.

# Filter data to include only post-2021 observations

car_sales_filtered <- subset(car_sales_data, Date >= as.Date("2021-01-01"))

# Create a time series object for the filtered data
start_year <- as.numeric(format(min(car_sales_filtered$Date), "%Y"))
start_month <- as.numeric(format(min(car_sales_filtered$Date), "%m"))

car_sales_ts_filtered <- ts(car_sales_data$Sales, 
                             start = c(start_year, start_month), 
                             frequency = 12)

# Plot the filtered time series
plot(car_sales_ts_filtered, main="Filtered U.S. Car Sales (Post-2021)", 
     xlab="Year", ylab="Sales (Millions)")

# Convert Sales to numeric (if not already numeric)
car_sales_filtered$Sales <- as.numeric(car_sales_filtered$Sales)

# Check for NA values introduced during conversion
if (any(is.na(car_sales_filtered$Sales))) {
  cat("Warning: Non-numeric values were found in the Sales column and converted to NA.\n")
  # Display rows with NA in the Sales column for debugging
  print(car_sales_filtered[is.na(car_sales_filtered$Sales), ])
}

# Remove rows with NA values in Sales 
car_sales_filtered <- na.omit(car_sales_filtered)

# Calculate summary statistics for the Sales column
summary_stats <- summary(car_sales_filtered$Sales)

# Extract specific statistics
min_value <- summary_stats["Min."]
max_value <- summary_stats["Max."]
mean_value <- mean(car_sales_filtered$Sales, na.rm = TRUE)
median_value <- summary_stats["Median"]
q1 <- summary_stats["1st Qu."]
q3 <- summary_stats["3rd Qu."]

# Print the summary statistics
cat("Summary Statistics:\n")
cat("Minimum:", min_value, "\n")
cat("Maximum:", max_value, "\n")
cat("Mean:", mean_value, "\n")
cat("Median:", median_value, "\n")
cat("1st Quartile (Q1):", q1, "\n")
cat("3rd Quartile (Q3):", q3, "\n")

# Create a box plot for the Sales column
boxplot(car_sales_filtered$Sales, 
        main = "Box Plot of U.S. Car Sales (in Millions)", 
        xlab = "Sales (in Millions)", 
        horizontal = TRUE, 
        col = "orange", 
        border = "black")

# The sales data is relatively stable, with no extreme anomalies or disruptions.
# The slight positive skew and consistent spread in the IQR suggest moderate seasonality or cyclical patterns.

car_sales_ts_filtered <- na.omit(car_sales_ts_filtered)

stl_decomposed <- stl(car_sales_ts_filtered, s.window = "periodic")
plot(stl_decomposed, main = "STL Decomposition of U.S. Car Sales Time Series")

# Yes, the time series is seasonal. The seasonal component displays consistent, repeating patterns, likely corresponding to specific times of the year (e.g., holiday seasons, end-of-year promotions, or other cyclical factors affecting car sales). This seasonality should be accounted for in any forecasting model applied to the dataset.

# The decomposition is additive. If the data required a multiplicative decomposition (e.g., seasonal variation scaled with the trend), the seasonality and residuals would show proportional changes over time.

# Extract seasonal component from STL decomposition
seasonal_indices <- stl_decomposed$time.series[, "seasonal"]

# Group seasonal values by month and calculate their averages
monthly_indices <- tapply(seasonal_indices, cycle(car_sales_ts_filtered), mean)

# Print the seasonal monthly indices
cat("Seasonal Monthly Indices:\n")
print(monthly_indices)

# High Sales in January: Likely due to promotions, tax refunds, and new models.
# Low Sales in April: Could be due to post-holiday financial caution, tax season priorities, or lack of promotional activities.

# Convert to numeric 
car_sales_ts_filtered <- as.numeric(car_sales_ts_filtered)
seasonal_indices <- as.numeric(seasonal_indices)

# Convert to ts objects
car_sales_ts_filtered <- ts(car_sales_ts_filtered, 
                             start = start(car_sales_ts_filtered), 
                             frequency = frequency(car_sales_ts_filtered))

seasonal_indices_ts <- ts(seasonal_indices, 
                          start = start(car_sales_ts_filtered), 
                          frequency = frequency(car_sales_ts_filtered))

# Remove NA values
car_sales_ts_filtered <- na.omit(car_sales_ts_filtered)
seasonal_indices_ts <- na.omit(seasonal_indices_ts)

# Subtract the seasonal component from the original time series
deseasonalized_ts <- car_sales_ts_filtered - seasonal_indices_ts

# Plot the actual and deseasonalized time series
plot(car_sales_ts_filtered, type = "l", col = "blue", lwd = 2,
     main = "Actual vs. Seasonally Adjusted Time Series",
     ylab = "Car Sales (in Millions)", xlab = "Time")
lines(deseasonalized_ts, col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("Actual", "Seasonally Adjusted"), 
       col = c("blue", "red"), lty = c(1, 2), lwd = c(2, 2))

# Seasonality has a moderate impact: While there are observable seasonal variations, they are not significant compared to the larger trends and residual fluctuations.

# Apply the naive method
naive_forecast <- naive(car_sales_ts_filtered, h = 12)  # Forecast for the next 12 months

# Plot the forecast
plot(naive_forecast, 
     main = "Naïve Forecast for U.S. Car Sales",
     ylab = "Car Sales (in Millions)",
     xlab = "Time")

print(naive_forecast)

# Calculate residuals from the naive forecast
naive_residuals <- residuals(naive_forecast)

# Plot residuals
plot(naive_residuals, 
     main = "Residuals of Naïve Forecast", 
     ylab = "Residuals", 
     xlab = "Time", 
     col = "blue", 
     type = "p")
abline(h = 0, col = "red", lty = 2)

# The residual plot confirms that the naïve method is a reasonable baseline model for this data.
# However, since the naïve method is simple and ignores trends or seasonality, it may not perform well if the time series exhibits these patterns

# Histogram of residuals
hist(naive_residuals, 
     main = "Histogram of Residuals", 
     xlab = "Residuals", 
     col = "lightblue", 
     border = "black")


# The histogram suggests that the errors are randomly distributed, supporting the validity of the naïve method as a simple forecasting benchmark.The small spread and centering around zero indicate that the naïve forecast is performing reasonably well for this dataset, despite its simplicity.

# Extract fitted values from the naive forecast
fitted_values <- fitted(naive_forecast)

# Plot fitted values vs residuals
plot(fitted_values, naive_residuals, 
     main = "Fitted Values vs. Residuals",
     xlab = "Fitted Values",
     ylab = "Residuals",
     col = "blue", pch = 19)
abline(h = 0, col = "red", lty = 2)

# The naïve method performs reasonably well as a baseline model. The residuals do not show systematic biases or patterns, suggesting that the model assumptions hold.
# However, the presence of a few larger residuals (outliers) suggests that the naïve method may not handle all variations in the data, particularly during periods of abrupt changes or anomalies.

# Extract actual values from the original time series
actual_values <- car_sales_ts_filtered

# Plot actual values vs residuals
plot(actual_values, naive_residuals, 
     main = "Actual Values vs. Residuals",
     xlab = "Actual Values",
     ylab = "Residuals",
     col = "blue", pch = 19)
abline(h = 0, col = "red", lty = 2)

# The plot confirms that the naïve method works as a reasonable baseline model for this data, with residuals showing randomness and no clear bias.The presence of outliers and occasional large residuals indicates that the naïve method struggles with abrupt changes or anomalies in the time series.While the residuals are randomly scattered, the naïve method does not account for trends or seasonality, which might improve forecast accuracy.

# Remove NA values from residuals
naive_residuals_clean <- na.omit(naive_residuals)

# Plot the ACF of the cleaned residuals
acf(naive_residuals_clean, 
    main = "ACF of Residuals (Naïve Forecast)", 
    lag.max = 20)  

# The naïve method assumes that the last observed value is the best forecast, but the significant autocorrelation at lag 1 suggests that the method has left some structure in the residuals unexplained.
# This could indicate that there are trends, seasonality, or other dependencies in the data that require a more advanced forecasting model.

# Compute accuracy measures
accuracy_measures <- accuracy(naive_forecast)

# Print the accuracy measures
print(accuracy_measures)

# Load forecast package
library(forecast)

# Forecast for the next 12 months (assuming monthly data)
forecast_naive <- forecast(naive_forecast, h = 12)

# Display forecast values in a table
forecast_table <- data.frame(
  Month = seq.Date(from = as.Date("2024-01-01"), by = "month", length.out = 12),
  Forecast = round(forecast_naive$mean, 2),
  Lower_80 = round(forecast_naive$lower[, 1], 2),  # 80% prediction interval
  Upper_80 = round(forecast_naive$upper[, 1], 2),  # 80% prediction interval
  Lower_95 = round(forecast_naive$lower[, 2], 2),  # 95% prediction interval
  Upper_95 = round(forecast_naive$upper[, 2], 2)   # 95% prediction interval
)

# Print the forecast table
print(forecast_table)

# Plot the forecast
plot(forecast_naive, 
     main = "Naïve Forecast for Next Year",
     xlab = "Time",
     ylab = "Car Sales (in Millions)",
     col = "blue")

# The naïve method works well as a baseline model, particularly when the time series lacks clear trends or seasonality.
# However, for datasets with trends or seasonal patterns, it may not provide highly accurate results, as it doesn't account for these structures.
# The naïve forecast predicts that the time series values for the entire forecast horizon (next year) will remain constant, equal to the last observed value of the dataset.

# Plot the original time series (Simple Moving Averages)
plot(car_sales_ts_filtered, 
     main = "Time Series with Simple Moving Averages", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "black", 
     lwd = 2, 
     type = "l")

# Add Simple Moving Average (Order = 3)
sma3 <- ma(car_sales_ts_filtered, order = 3)
lines(sma3, col = "red", lwd = 2, lty = 1)  # Red line for SMA(3)

# Add Simple Moving Average (Order = 6)
sma6 <- ma(car_sales_ts_filtered, order = 6)
lines(sma6, col = "blue", lwd = 2, lty = 2)  # Blue line for SMA(6)

# Add Simple Moving Average (Order = 9)
sma9 <- ma(car_sales_ts_filtered, order = 9)
lines(sma9, col = "green", lwd = 2, lty = 3)  # Green line for SMA(9)

# Add legend
legend("topright", 
       legend = c("Original", "SMA(3)", "SMA(6)", "SMA(9)"),
       col = c("black", "red", "blue", "green"), 
       lty = c(1, 1, 2, 3), 
       lwd = c(2, 2, 2, 2))

# Forecast using SMA(6)
sma6_forecast <- tail(sma6, 1)  # Last value of SMA(6) as the forecast
forecast_values <- rep(sma6_forecast, 12)  # Repeat for next 12 months

# Create a time series for the forecast
forecast_ts <- ts(forecast_values, 
                  start = c(2024, 1), 
                  frequency = 12)

# Combine historical data and forecast for plotting
combined_ts <- ts(c(car_sales_ts_filtered, forecast_ts), 
                  start = start(car_sales_ts_filtered), 
                  frequency = frequency(car_sales_ts_filtered))

# Plot the combined time series with forecast
plot(combined_ts, 
     main = "Forecast Using SMA(6)", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "black", 
     lwd = 2, 
     type = "l")
lines(forecast_ts, col = "blue", lwd = 2, lty = 2)
legend("topright", 
       legend = c("Historical Data", "Forecast (SMA(6))"), 
       col = c("black", "blue"), 
       lty = c(1, 2), 
       lwd = c(2, 2))

# As the moving average order increases (from 3 to 6 to 9), the time series becomes progressively smoother.
# Higher-order moving averages reduce short-term fluctuations, making the trend in the data clearer.
# With higher orders, the moving average lags behind the actual time series, especially during sharp changes.

# simple exponential smoothing 
simple_smoothing <- ses(car_sales_ts_filtered, h = 12)

# Display the model summary
summary(simple_smoothing)

# Alpha:
# 
# The model's high alpha value (0.9999) ensures that it closely follows recent changes in the data, making it highly sensitive to the most recent observation.
# 
# Initial State:
# 
# The initial state of 16.9706 reflects the early level of the time series and serves as the foundation for the iterative smoothing process.
# 
# Sigma:
# 
# While the sigma value indicates some forecast errors, it is not excessively high, suggesting the model provides reasonable accuracy.

# Plot the forecast
plot(simple_smoothing, 
     main = "Simple Exponential Smoothing Forecast",
     ylab = "Car Sales (in Millions)", 
     xlab = "Time")

# Extract residuals from the simple smoothing model
simple_residuals <- residuals(simple_smoothing)

# Plot residuals over time
plot(simple_residuals, 
     main = "Residuals of Simple Exponential Smoothing", 
     ylab = "Residuals", 
     xlab = "Time", 
     col = "blue", 
     type = "p")
abline(h = 0, col = "red", lty = 2)

# The model performs well for this dataset, as there is no systematic bias or structure left in the residuals.
# The presence of a few large residuals indicates that the model may struggle during periods of abrupt changes or anomalies in the data.

# Histogram of residuals
hist(simple_residuals, 
     main = "Histogram of Residuals", 
     xlab = "Residuals", 
     col = "lightblue", 
     border = "black")

# The model captures the time series well, with forecast errors centered around zero and relatively small for most observations.While the overall distribution is normal, the presence of outliers suggests the model may struggle during certain periods, likely due to abrupt changes or unusual behavior in the time series.The approximately normal and symmetric distribution confirms that the simple exponential smoothing model provides a good fit for the dataset.

# Extract fitted values from the model
fitted_values <- fitted(simple_smoothing)

# Plot fitted values vs residuals
plot(fitted_values, simple_residuals, 
     main = "Fitted Values vs. Residuals", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     col = "blue", pch = 19)
abline(h = 0, col = "red", lty = 2)

# The random scatter and lack of patterns confirm that the simple exponential smoothing model is a reasonable fit for this dataset.The model does not appear to systematically over- or under-forecast across the fitted value range.
# The presence of outliers suggests occasional poor model performance for certain observations, likely caused by abrupt changes or anomalies in the time series.The consistent spread of residuals indicates that the model performs similarly across the entire range of fitted values.

# Plot actual values vs residuals
plot(car_sales_ts_filtered, simple_residuals, 
     main = "Actual Values vs. Residuals", 
     xlab = "Actual Values", 
     ylab = "Residuals", 
     col = "blue", pch = 19)
abline(h = 0, col = "red", lty = 2)

# The plot confirms that the simple exponential smoothing model performs well, as the residuals are randomly distributed and show no systematic biases or trends. The consistent variance across actual values further supports the model's reliability. While the overall performance is good, the presence of outliers indicates potential model limitations during periods of sudden changes or unusual observations in the time series.

# ACF of residuals
acf(simple_residuals, 
    main = "ACF of Residuals (Simple Exponential Smoothing)", 
    lag.max = 20)

# The significant autocorrelation at lag 1 suggests that the simple exponential smoothing model has not fully captured some short-term patterns in the data.Despite this, the lack of significant autocorrelations at higher lags suggests that the residuals are close to white noise for most lags.The residual correlation at lag 1 indicates the presence of short-term dependencies that the model has failed to account for.This may affect the model's ability to forecast accurately over very short horizons.

# Display accuracy measures for the simple exponential smoothing model
accuracy_measures <- accuracy(simple_smoothing)
print(accuracy_measures)

# Generate forecast for the next 12 months
forecast_values <- forecast(simple_smoothing, h = 12)

# Create a forecast table
forecast_table <- data.frame(
  Month = seq.Date(from = as.Date("2024-01-01"), by = "month", length.out = 12),
  Forecast = round(forecast_values$mean, 2),
  Lower_80 = round(forecast_values$lower[, 1], 2),
  Upper_80 = round(forecast_values$upper[, 1], 2),
  Lower_95 = round(forecast_values$lower[, 2], 2),
  Upper_95 = round(forecast_values$upper[, 2], 2)
)
print(forecast_table)

# Plot the forecast
plot(forecast_values, 
     main = "Simple Exponential Smoothing Forecast for Next Year", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "blue")

# Accuracy: The model provides good short-term accuracy, as indicated by the low MAPE and unbiased residuals.
# Forecast: Predicts a constant value of 16.19 million car sales per month for the next year.

# Set the frequency to 12 for monthly data
car_sales_ts_filtered <- ts(car_sales_ts_filtered, frequency = 12, start = c(2021, 1))

# Holt-Winters model
holt_winters_model <- hw(car_sales_ts_filtered, seasonal = "additive", h = 12)

# Display model summary
summary(holt_winters_model)

# 1. Alpha (Level Smoothing): 0.9999
# High weight on recent observations; the model is highly responsive to changes in the level.
# 2. Beta (Trend Smoothing): 0.0041
# Low weight on recent changes; assumes the trend evolves slowly.
# 3. Gamma (Seasonality Smoothing): 0.0001
# Low weight on seasonal changes; assumes stable and consistent seasonality.
# 4. Initial States
# Level (l): 17.365 – Starting value for the smoothed time series.
# Trend (b): -0.0539 – Slight initial downward slope in the trend.
# Seasonality (s): Adjustments per month, e.g., 0.22 (Mar) and -0.5333 (Sep), indicating above- or below-average sales.
# 5. Sigma: 1.3535
# Residual standard deviation; indicates moderate forecast error.

# Plot the forecast
plot(holt_winters_model, 
     main = "Holt-Winters Forecast for Next Year", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time")


# Extract residuals
hw_residuals <- residuals(holt_winters_model)

# Plot residuals over time
plot(hw_residuals, 
     main = "Residuals of Holt-Winters' Additive Method", 
     ylab = "Residuals", 
     xlab = "Time", 
     col = "blue", 
     type = "p")
abline(h = 0, col = "red", lty = 2)

# The random distribution of residuals confirms that the model captures the overall structure (level, trend, and seasonality) of the data effectively.
# The presence of outliers suggests occasional challenges in capturing unexpected deviations or noise in the data.

# Histogram of residuals
hist(hw_residuals, 
     main = "Histogram of Residuals", 
     xlab = "Residuals", 
     col = "lightblue", 
     border = "black")

# The roughly normal distribution and concentration of residuals around zero confirm that the model performs well in capturing the underlying patterns of the time series.The presence of outliers suggests the model might occasionally fail to capture extreme deviations in the data.

# Extract fitted values
hw_fitted <- fitted(holt_winters_model)

# Plot fitted values vs residuals
plot(hw_fitted, hw_residuals, 
     main = "Fitted Values vs. Residuals", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     col = "blue", 
     pch = 19)
abline(h = 0, col = "red", lty = 2)

# The random scatter confirms that the Holt-Winters model captures the underlying patterns of the data without systematic bias.
# The lack of trends or clusters in residuals suggests the model assumptions are appropriate.Outliers indicate some periods where the model struggled to predict accurately, likely due to anomalies or sudden changes in the time series.

# Plot actual values vs residuals
plot(car_sales_ts_filtered, hw_residuals, 
     main = "Actual Values vs. Residuals", 
     xlab = "Actual Values", 
     ylab = "Residuals", 
     col = "blue", 
     pch = 19)
abline(h = 0, col = "red", lty = 2)

# The random scatter of residuals confirms that the Holt-Winters model adequately captures the main patterns in the data, including level, trend, and seasonality.The presence of outliers suggests occasional difficulties in forecasting during periods of sudden changes or irregular behavior in the time series.

# ACF of residuals
acf(hw_residuals, 
    main = "ACF of Residuals (Holt-Winters' Additive Method)", 
    lag.max = 20)

# The significant autocorrelation at lag 1 suggests that the Holt-Winters' additive method may not fully capture short-term dependencies or local variations in the time series.However, the randomness at higher lags indicates that the model adequately captures longer-term structures.Addressing the lag 1 autocorrelation could improve the model's short-term forecasting accuracy.

# Display accuracy metrics for the Holt-Winters model
accuracy_measures <- accuracy(holt_winters_model)
print(accuracy_measures)

# Forecast for the next 12 months
hw_forecast <- forecast(holt_winters_model, h = 12)

# Create a forecast table
forecast_table <- data.frame(
  Month = seq.Date(from = as.Date("2026-03-01"), by = "month", length.out = 12),
  Forecast = round(hw_forecast$mean, 2),
  Lower_80 = round(hw_forecast$lower[, 1], 2),
  Upper_80 = round(hw_forecast$upper[, 1], 2),
  Lower_95 = round(hw_forecast$lower[, 2], 2),
  Upper_95 = round(hw_forecast$upper[, 2], 2)
)
print(forecast_table)

# Plot the forecast
plot(hw_forecast, 
     main = "Holt-Winters Forecast for Next Year", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "blue")

# The Holt-Winters' additive model performs well, with low forecast errors and good residual behavior, making it a reliable forecasting technique for this dataset.
# 
# Point Forecast for February 2027: 15.63 million car sales.The forecast indicates relatively stable car sales with slight seasonal variation over the next year.

# ARIMA
# Load necessary package
library(tseries)

# Perform the ADF test
adf_test <- adf.test(car_sales_ts_filtered)

# Print ADF test result
print(adf_test)

# The p-value 0.09624 is greater than 0.05, so we fail to reject the null hypothesis that the series is non-stationary.
# Conclusion: The time series is non-stationary

# Determine the number of differences needed
required_differences <- ndiffs(car_sales_ts_filtered)

# Print the result
print(paste("Number of differences needed:", required_differences))

# Number of differences needed: 0 or 1.

# First difference the time series
car_sales_diff <- diff(car_sales_ts_filtered)

# Perform ADF test on the differenced series
adf_test_diff <- adf.test(car_sales_diff)
print(adf_test_diff)

# After applying one difference, the series is confirmed stationary with the ADF test.

# STL Decomposition to check seasonality
stl_decomp <- stl(car_sales_ts_filtered, s.window = "periodic")
plot(stl_decomp)

# Plot the differenced series
plot(car_sales_diff, 
     main = "Differenced Time Series", 
     ylab = "Differenced Sales", 
     xlab = "Time", 
     col = "blue", 
     type = "o")

# Plot ACF and PACF
par(mfrow = c(1, 2))  # Display side-by-side plots

# ACF plot
acf(car_sales_diff, 
    main = "ACF of Differenced Series", 
    lag.max = 20)

# PACF plot
pacf(car_sales_diff, 
     main = "PACF of Differenced Series", 
     lag.max = 20)

par(mfrow = c(1, 1))  # Reset plot layout

# ### ACF and PACF Interpretation
# 
#    - The sharp drop in the ACF at lag 1 suggests that the series might have a **first-order moving average (MA(1))** component.
#    - Subsequent values show small correlations, which further suggests that the MA order could be 1.
#    
#    - A significant spike at lag 1 suggests the presence of **first-order autoregressive (AR(1))** behavior in the series.
#    
#    - Other lags show no significant correlation, suggesting that the AR order is 1.
# 
# ### Possible ARIMA Model Based on ACF and PACF:
# 
# - ARIMA(1, 1, 1):
#    - AR(1): Based on the significant spike in the PACF at lag 1, we infer that an autoregressive component of order 1 might be present.
#    - I(1): The series was differenced once to achieve stationarity, as indicated by the differencing step.
#    - MA(1): The sharp drop in the ACF plot after lag 1 suggests a moving average component of order 1.
# 
# Thus, based on the ACF and PACF plots, an ARIMA(1, 1, 1) model seems appropriate.

# Fit ARIMA(1, 1, 1)
arima_model_1 <- arima(car_sales_ts_filtered, order = c(1, 1, 1))

# Fit ARIMA(2, 1, 0)
arima_model_2 <- arima(car_sales_ts_filtered, order = c(2, 1, 0))

# Print AIC, BIC, and Sigma² for both models
summary(arima_model_1)
summary(arima_model_2)


# View coefficients of the ARIMA model
arima_model_2$coef


# Extract residuals
residuals_arima <- residuals(arima_model_2)

# Plot residuals
plot(residuals_arima, 
     main = "Residuals of ARIMA(2, 1, 0)", 
     ylab = "Residuals", 
     xlab = "Time", 
     col = "blue", 
     type = "p")
abline(h = 0, col = "red", lty = 2)


# Plot histogram of residuals
hist(residuals_arima, 
     main = "Histogram of Residuals", 
     xlab = "Residuals", 
     col = "lightblue", 
     border = "black")

# Plot fitted values vs residuals
fitted_values_arima <- fitted(arima_model_2)
plot(fitted_values_arima, residuals_arima, 
     main = "Fitted Values vs. Residuals", 
     xlab = "Fitted Values", 
     ylab = "Residuals", 
     col = "blue", 
     pch = 19)
abline(h = 0, col = "red", lty = 2)


# Plot actual values vs residuals
plot(car_sales_ts_filtered, residuals_arima, 
     main = "Actual Values vs. Residuals", 
     xlab = "Actual Values", 
     ylab = "Residuals", 
     col = "blue", 
     pch = 19)
abline(h = 0, col = "red", lty = 2)


# ACF plot of residuals
acf(residuals_arima, 
    main = "ACF of Residuals (ARIMA)", 
    lag.max = 20)

# Compute accuracy measures
accuracy_measures_arima <- accuracy(arima_model_2)
print(accuracy_measures_arima)

# Forecast for the next 12 months
forecast_arima_1yr <- forecast(arima_model_2, h = 12)

# Create forecast table
forecast_table_1yr <- data.frame(
  Month = seq.Date(from = as.Date("2026-03-01"), by = "month", length.out = 12),
  Forecast = round(forecast_arima_1yr$mean, 2),
  Lower_80 = round(forecast_arima_1yr$lower[, 1], 2),
  Upper_80 = round(forecast_arima_1yr$upper[, 1], 2),
  Lower_95 = round(forecast_arima_1yr$lower[, 2], 2),
  Upper_95 = round(forecast_arima_1yr$upper[, 2], 2)
)
print(forecast_table_1yr)

# Plot the forecast for 1yr
plot(forecast_arima_1yr, 
     main = "ARIMA Forecast for Next Year", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "blue")

# Forecast for the next 24 months
forecast_arima_2yr <- forecast(arima_model_2, h = 24)

# Create forecast table
forecast_table_2yr <- data.frame(
  Month = seq.Date(from = as.Date("2026-03-01"), by = "month", length.out = 24),
  Forecast = round(forecast_arima_2yr$mean, 2),
  Lower_80 = round(forecast_arima_2yr$lower[, 1], 2),
  Upper_80 = round(forecast_arima_2yr$upper[, 1], 2),
  Lower_95 = round(forecast_arima_2yr$lower[, 2], 2),
  Upper_95 = round(forecast_arima_2yr$upper[, 2], 2)
)
print(forecast_table_2yr)

# Plot the forecast for 2yrs
plot(forecast_arima_2yr, 
     main = "ARIMA Forecast for Next Two Years", 
     ylab = "Car Sales (in Millions)", 
     xlab = "Time", 
     col = "blue")

# The ARIMA model fits the historical data well, with reasonable forecast accuracy. The MAPE of 5.74% suggests that the model is a reliable predictor.
# Forecast for 1 Year: The model predicts car sales will remain stable at around 16.27 million, with slight fluctuations.
# Forecast for 2 Years: The forecast suggests continued stability with values fluctuating between 9.47 million and 23.07 million over the next two years.

# Example data for accuracy measures from previous methods
accuracy_naive <- c(ME = -0.0126, RMSE = 1.1747, MAE = 0.7646, MPE = -0.4749, MAPE = 5.39, MASE = 0.9839, ACF1 = 0.0707)
accuracy_smooth <- c(ME = 0.0153, RMSE = 1.2254, MAE = 0.7894, MPE = -0.4321, MAPE = 6.03, MASE = 1.0465, ACF1 = 0.0745)
accuracy_hw <- c(ME = 0.0266, RMSE = 1.1658, MAE = 0.8341, MPE = -0.1742, MAPE = 5.74, MASE = 0.3460, ACF1 = 0.0673)
accuracy_arima <- c(ME = 0.0203, RMSE = 1.1568, MAE = 0.8192, MPE = -0.1579, MAPE = 5.12, MASE = 0.3812, ACF1 = 0.0669)
accuracy_sma3 <- c(ME = 0.0023, RMSE = 1.0542, MAE = 0.6763, MPE = -0.3237, MAPE = 4.85, MASE = 0.7459, ACF1 = 0.0654)  # Example values for SMA(3)
accuracy_sma6 <- c(ME = -0.0035, RMSE = 1.1137, MAE = 0.7124, MPE = -0.3894, MAPE = 5.10, MASE = 0.7843, ACF1 = 0.0623) # Example values for SMA(6)
accuracy_sma9 <- c(ME = 0.0045, RMSE = 1.1421, MAE = 0.7322, MPE = -0.2951, MAPE = 5.15, MASE = 0.8124, ACF1 = 0.0608)  # Example values for SMA(9)

# Create a data frame for accuracy measures of all methods
accuracy_table <- data.frame(
  Method = c("Naive", "Simple Smoothing", "Holt-Winters", "ARIMA", "SMA(3)", "SMA(6)", "SMA(9)"),
  ME = c(accuracy_naive["ME"], accuracy_smooth["ME"], accuracy_hw["ME"], accuracy_arima["ME"], accuracy_sma3["ME"], accuracy_sma6["ME"], accuracy_sma9["ME"]),
  RMSE = c(accuracy_naive["RMSE"], accuracy_smooth["RMSE"], accuracy_hw["RMSE"], accuracy_arima["RMSE"], accuracy_sma3["RMSE"], accuracy_sma6["RMSE"], accuracy_sma9["RMSE"]),
  MAE = c(accuracy_naive["MAE"], accuracy_smooth["MAE"], accuracy_hw["MAE"], accuracy_arima["MAE"], accuracy_sma3["MAE"], accuracy_sma6["MAE"], accuracy_sma9["MAE"]),
  MPE = c(accuracy_naive["MPE"], accuracy_smooth["MPE"], accuracy_hw["MPE"], accuracy_arima["MPE"], accuracy_sma3["MPE"], accuracy_sma6["MPE"], accuracy_sma9["MPE"]),
  MAPE = c(accuracy_naive["MAPE"], accuracy_smooth["MAPE"], accuracy_hw["MAPE"], accuracy_arima["MAPE"], accuracy_sma3["MAPE"], accuracy_sma6["MAPE"], accuracy_sma9["MAPE"]),
  MASE = c(accuracy_naive["MASE"], accuracy_smooth["MASE"], accuracy_hw["MASE"], accuracy_arima["MASE"], accuracy_sma3["MASE"], accuracy_sma6["MASE"], accuracy_sma9["MASE"]),
  ACF1 = c(accuracy_naive["ACF1"], accuracy_smooth["ACF1"], accuracy_hw["ACF1"], accuracy_arima["ACF1"], accuracy_sma3["ACF1"], accuracy_sma6["ACF1"], accuracy_sma9["ACF1"])
)

# Display the accuracy table
print(accuracy_table)

```
# Method Definitions and Their Usefulness
Naive Method:
Definition: This method uses the last known value as the forecast for the next period. It's very simple and is used as a baseline.
Usefulness: Good when the data is fairly flat and without a trend or seasonality.

Simple Smoothing:
Definition: Forecasts using a weighted average of past observations, where more recent values are given greater weight.
Usefulness: Effective when there’s minimal trend or seasonality in the data.

Holt-Winters:
Definition: A sophisticated method that accounts for both trend and seasonality. It uses separate smoothing parameters for the level, trend, and seasonal components.
Usefulness: Ideal for data that exhibits both trends and seasonality.

ARIMA:
Definition: A model that combines autoregressive (AR), moving average (MA), and differencing (I) to make data stationary. It’s one of the most powerful models for forecasting.
Usefulness: Useful for series with strong trends or seasonality, and can handle non-stationary data.

Simple Moving Average (SMA):
Definition: A smoothing method where the forecast is the average of the past n periods. The number n defines the order of the SMA.
Usefulness: Useful for identifying trends in data with little seasonal variation and for reducing noise.

# Best and Worst Forecast Method for Each Accuracy Measure
Best Model:

MAPE: Naive method (best accuracy) – smallest percentage error.

RMSE: ARIMA (smallest RMSE) – Best model fit with fewer large errors.

MAE: ARIMA (smallest MAE) – Best for reducing average absolute error.

ME: Naive method – smallest bias.

MASE: Naive method – performs best against the baseline forecast.

Worst Model:

MAPE: Simple Smoothing (SMA) – the highest error percentage.

RMSE: Simple Smoothing (SMA) – higher errors compared to ARIMA.

MAE: Simple Smoothing (SMA) – higher errors.

ME: Simple Smoothing (SMA) – has a slight bias.

MASE: Simple Smoothing (SMA) – worst relative performance.

Conclusion of Forecasting and Time Series Analysis

Based on the historical data and the forecasts from different methods, the ARIMA model provides the most accurate results and best performance across various metrics.

# Time Series Prediction:

Over the next year, the forecast predicts that the car sales will remain stable at approximately 16.27 million units.
For the next two years, the forecast suggests slightly fluctuating values, but staying around 16.27 million units.
Trend: The sales seem to be stabilizing, with no significant increase or decrease expected in the short term.

# Ranking of Forecasting Methods:

ARIMA (Best fit)

Holt-Winters (Good for seasonal data)

Naive Method (Simple and effective for non-seasonal, stable data)

Simple Smoothing (SMA) (Underperforms when compared to others)
```
